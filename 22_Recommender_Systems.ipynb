{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "popular_interests = Counter(interest\n",
    "                            for user_interests in users_interests\n",
    "                            for interest in user_interests)\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def most_popular_new_interests(\n",
    "        user_interests: List[str],\n",
    "        max_results: int = 5) -> List[Tuple[str, int]]:\n",
    "    suggestions = [(interest, frequency)\n",
    "                   for interest, frequency in popular_interests.most_common()\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]\n",
    "\n",
    "unique_interests = sorted({interest\n",
    "                           for user_interests in users_interests\n",
    "                           for interest in user_interests})\n",
    "\n",
    "assert unique_interests[:6] == [\n",
    "    'Big Data',\n",
    "    'C++',\n",
    "    'Cassandra',\n",
    "    'HBase',\n",
    "    'Hadoop',\n",
    "    'Haskell',\n",
    "    # ...\n",
    "]\n",
    "\n",
    "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a list ofinterests, produce a vector whose ith element is 1\n",
    "    if unique_interests[i] is in the list, 0 otherwise\n",
    "    \"\"\"\n",
    "    return [1 if interest in user_interests else 0\n",
    "            for interest in unique_interests]\n",
    "\n",
    "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
    "                         for user_interests in users_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2017.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Python and statistics 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "databases 3\n",
      "Big Data and programming languages 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nlp import cosine_similarity\n",
    "\n",
    "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
    "                      for interest_vector_j in user_interest_vectors]\n",
    "                     for interest_vector_i in user_interest_vectors]\n",
    "\n",
    "# Users 0 and 9 share interests in Hadoop, Java, and Big Data\n",
    "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\"\n",
    "\n",
    "# Users 0 and 8 share only one interest: Big Data\n",
    "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
    "    pairs = [(other_user_id, similarity)                      # Find other\n",
    "             for other_user_id, similarity in                 # users with\n",
    "                enumerate(user_similarities[user_id])         # nonzero\n",
    "             if user_id != other_user_id and similarity > 0]  # similarity.\n",
    "\n",
    "    return sorted(pairs,                                      # Sort them\n",
    "                  key=lambda pair: pair[-1],                  # most similar\n",
    "                  reverse=True)                               # first.\n",
    "\n",
    "\n",
    "most_similar_to_zero = most_similar_users_to(0)\n",
    "user, score = most_similar_to_zero[0]\n",
    "assert user == 9\n",
    "assert 0.56 < score < 0.57\n",
    "user, score = most_similar_to_zero[1]\n",
    "assert user == 1\n",
    "assert 0.33 < score < 0.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def user_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Sum up the similarities.\n",
    "    suggestions: Dict[str, float] = defaultdict(float)\n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity\n",
    "\n",
    "    # Convert them to a sorted list.\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[-1],  # weight\n",
    "                         reverse=True)\n",
    "\n",
    "    # And (maybe) exclude already-interests\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    "\n",
    "\n",
    "ubs0 = user_based_suggestions(0)\n",
    "interest, score = ubs0[0]\n",
    "assert interest == 'MapReduce'\n",
    "assert 0.56 < score < 0.57\n",
    "interest, score = ubs0[1]\n",
    "assert interest == 'MongoDB'\n",
    "assert 0.50 < score < 0.51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_user_matrix = [[user_interest_vector[j]\n",
    "                         for user_interest_vector in user_interest_vectors]\n",
    "                        for j, _ in enumerate(unique_interests)]\n",
    "\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
    "                          for user_vector_j in interest_user_matrix]\n",
    "                         for user_vector_i in interest_user_matrix]\n",
    "\n",
    "def most_similar_interests_to(interest_id: int):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs,\n",
    "                  key=lambda pair: pair[-1],\n",
    "                  reverse=True)\n",
    "\n",
    "\n",
    "msit0 = most_similar_interests_to(0)\n",
    "assert msit0[0][0] == 'Hadoop'\n",
    "assert 0.815 < msit0[0][1] < 0.817\n",
    "assert msit0[1][0] == 'Java'\n",
    "assert 0.666 < msit0[1][1] < 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Add up the similar interests\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_vectors[user_id]\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "\n",
    "    # Sort them by weight\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[-1],\n",
    "                         reverse=True)\n",
    "\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    "\n",
    "[('MapReduce', 1.861807319565799),\n",
    " ('Postgres', 1.3164965809277263),\n",
    " ('MongoDB', 1.3164965809277263),\n",
    " ('NoSQL', 1.2844570503761732),\n",
    " ('programming languages', 0.5773502691896258),\n",
    " ('MySQL', 0.5773502691896258),\n",
    " ('Haskell', 0.5773502691896258),\n",
    " ('databases', 0.5773502691896258),\n",
    " ('neural networks', 0.4082482904638631),\n",
    " ('deep learning', 0.4082482904638631),\n",
    " ('C++', 0.4082482904638631),\n",
    " ('artificial intelligence', 0.4082482904638631),\n",
    " ('Python', 0.2886751345948129),\n",
    " ('R', 0.2886751345948129)]\n",
    "\n",
    "\n",
    "ibs0 = item_based_suggestions(0)\n",
    "assert ibs0[0][0] == 'MapReduce'\n",
    "assert 1.86 < ibs0[0][1] < 1.87\n",
    "assert ibs0[1][0] in ('Postgres', 'MongoDB')  # A tie\n",
    "assert 1.31 < ibs0[1][1] < 1.32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected at least 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-834da1ffa932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#for row in reader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#    print(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create a list of [Rating]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-834da1ffa932>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#for row in reader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#    print(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create a list of [Rating]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected at least 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Replace this with the locations of your files\n",
    "\n",
    "# This points to the current directory, modify if your files are elsewhere.\n",
    "MOVIES = \"ml-25m/movies.csv\"   # pipe-delimited: movie_id|title|...\n",
    "RATINGS = \"ml-25m/ratings.csv\"  # tab-delimited: user_id, movie_id, rating, timestamp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Rating(NamedTuple):\n",
    "    user_id: str\n",
    "    movie_id: str\n",
    "    rating: float\n",
    "\n",
    "import csv\n",
    "# We specify this encoding to avoid a UnicodeDecodeError.\n",
    "# see: https://stackoverflow.com/a/53136168/1076346\n",
    "with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    next(reader, None)  # skip the headers  #movieId,title,genres#\n",
    "    #for row in reader:\n",
    "    #    print(row)\n",
    "    movies = {movie_id: title for movie_id, title, *_ in reader}\n",
    "print(movies)\n",
    "# Create a list of [Rating]\n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    next(reader, None)  # skip the headers\n",
    "    #for row in reader:\n",
    "    #    print(row)\n",
    "    ratings = [Rating(user_id, movie_id, float(rating))\n",
    "               for user_id, movie_id, rating, _ in reader]\n",
    "\n",
    "# 1682 movies rated by 943 users\n",
    "assert len(movies) == 1682\n",
    "assert len(list({rating.user_id for rating in ratings})) == 943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Data structure for accumulating ratings by movie_id\n",
    "star_wars_ratings = {movie_id: []\n",
    "                     for movie_id, title in movies.items()\n",
    "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
    "\n",
    "# Iterate over ratings, accumulating the Star Wars ones\n",
    "for rating in ratings:\n",
    "    if rating.movie_id in star_wars_ratings:\n",
    "        star_wars_ratings[rating.movie_id].append(rating.rating)\n",
    "\n",
    "# Compute the average rating for each movie\n",
    "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
    "               for movie_id, title_ratings in star_wars_ratings.items()]\n",
    "\n",
    "# And then print them in order\n",
    "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
    "    print(f\"{avg_rating:.2f} {movies[movie_id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(ratings)\n",
    "\n",
    "split1 = int(len(ratings) * 0.7)\n",
    "split2 = int(len(ratings) * 0.85)\n",
    "\n",
    "train = ratings[:split1]              # 70% of the data\n",
    "validation = ratings[split1:split2]   # 15% of the data\n",
    "test = ratings[split2:]               # 15% of the data\n",
    "\n",
    "avg_rating = sum(rating.rating for rating in train) / len(train)\n",
    "baseline_error = sum((rating.rating - avg_rating) ** 2\n",
    "                     for rating in test) / len(test)\n",
    "\n",
    "# This is what we hope to do better than\n",
    "assert 1.26 < baseline_error < 1.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding vectors for matrix factorization model\n",
    "\n",
    "from scratch.deep_learning import random_tensor\n",
    "\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# Find unique ids\n",
    "user_ids = {rating.user_id for rating in ratings}\n",
    "movie_ids = {rating.movie_id for rating in ratings}\n",
    "\n",
    "# Then create a random vector per id\n",
    "user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
    "                for user_id in user_ids}\n",
    "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
    "                 for movie_id in movie_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for matrix factorization model\n",
    "\n",
    "from typing import List\n",
    "import tqdm\n",
    "from scratch.linear_algebra import dot\n",
    "\n",
    "def loop(dataset: List[Rating],\n",
    "         learning_rate: float = None) -> None:\n",
    "    with tqdm.tqdm(dataset) as t:\n",
    "        loss = 0.0\n",
    "        for i, rating in enumerate(t):\n",
    "            movie_vector = movie_vectors[rating.movie_id]\n",
    "            user_vector = user_vectors[rating.user_id]\n",
    "            predicted = dot(user_vector, movie_vector)\n",
    "            error = predicted - rating.rating\n",
    "            loss += error ** 2\n",
    "\n",
    "            if learning_rate is not None:\n",
    "                #     predicted = m_0 * u_0 + ... + m_k * u_k\n",
    "                # So each u_j enters output with coefficent m_j\n",
    "                # and each m_j enters output with coefficient u_j\n",
    "                user_gradient = [error * m_j for m_j in movie_vector]\n",
    "                movie_gradient = [error * u_j for u_j in user_vector]\n",
    "\n",
    "                # Take gradient steps\n",
    "                for j in range(EMBEDDING_DIM):\n",
    "                    user_vector[j] -= learning_rate * user_gradient[j]\n",
    "                    movie_vector[j] -= learning_rate * movie_gradient[j]\n",
    "\n",
    "            t.set_description(f\"avg loss: {loss / (i + 1)}\")\n",
    "\n",
    "learning_rate = 0.05\n",
    "for epoch in range(20):\n",
    "    learning_rate *= 0.9\n",
    "    print(epoch, learning_rate)\n",
    "    loop(train, learning_rate=learning_rate)\n",
    "    loop(validation)\n",
    "loop(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.working_with_data import pca, transform\n",
    "\n",
    "original_vectors = [vector for vector in movie_vectors.values()]\n",
    "components = pca(original_vectors, 2)\n",
    "\n",
    "ratings_by_movie = defaultdict(list)\n",
    "for rating in ratings:\n",
    "    ratings_by_movie[rating.movie_id].append(rating.rating)\n",
    "\n",
    "vectors = [\n",
    "    (movie_id,\n",
    "     sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n",
    "     movies[movie_id],\n",
    "     vector)\n",
    "    for movie_id, vector in zip(movie_vectors.keys(),\n",
    "                                transform(original_vectors, components))\n",
    "]\n",
    "\n",
    "# Print top 25 and bottom 25 by first principal component\n",
    "print(sorted(vectors, key=lambda v: v[-1][0])[:25])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
