{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)\n",
    "\n",
    "import random\n",
    "\n",
    "from probability import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]\n",
    "\n",
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())\n",
    "\n",
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SSE(Loss):\n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Compute the tensor of squared differences\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "        # And just add them up\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual),\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "\n",
    "sse_loss = SSE()\n",
    "assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n",
    "assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer (in place) using information\n",
    "    known by either the layer or the optimizer (or by both).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad)\n",
    "\n",
    "tensor = [[1, 2], [3, 4]]\n",
    "\n",
    "for row in tensor:\n",
    "    row = [0, 0]\n",
    "assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n",
    "\n",
    "for row in tensor:\n",
    "    row[:] = [0, 0]\n",
    "assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates: List[Tensor] = []  # running average\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        # If we have no previous updates, start with all zeros.\n",
    "        if not self.updates:\n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
    "\n",
    "        for update, param, grad in zip(self.updates,\n",
    "                                       layer.params(),\n",
    "                                       layer.grads()):\n",
    "            # Apply momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad)\n",
    "\n",
    "            # Then take a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because e.g. math.exp(1000) raises an error.\n",
    "    if x < -100:  return -1\n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save tanh output to use in backward pass.\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
    "            self.tanh,\n",
    "            gradient)\n",
    "\n",
    "class Relu(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x: max(x, 0), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,\n",
    "                              gradient)\n",
    "\n",
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stabilitity.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \"\"\"\n",
    "    This is the negative-log-likelihood of the observed values, given the\n",
    "    neural net model. So if we choose weights to minimize it, our model will\n",
    "    be maximizing the likelihood of the observed data.\n",
    "    \"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # This will be log p_i for the actual class i and 0 for the other\n",
    "        # classes. We add a tiny amount to p to avoid taking log(0).\n",
    "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
    "                                     probabilities,\n",
    "                                     actual)\n",
    "\n",
    "        # And then we just sum up the negatives.\n",
    "        return -tensor_sum(likelihoods)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # Isn't this a pleasant equation?\n",
    "        return tensor_combine(lambda p, actual: p - actual,\n",
    "                              probabilities,\n",
    "                              actual)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")\n",
    "\n",
    "\n",
    "#plt.savefig('im/mnist.png')\n",
    "#plt.gca().clear()\n",
    "\n",
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
    "\n",
    "assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import squared_distance\n",
    "\n",
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)\n",
    "\n",
    "def load_weights(model: Layer, filename: str) -> None:\n",
    "    with open(filename) as f:\n",
    "        weights = json.load(f)\n",
    "\n",
    "    # Check for consistency\n",
    "    assert all(shape(param) == shape(weight)\n",
    "               for param, weight in zip(model.params(), weights))\n",
    "\n",
    "    # Then load using slice assignment:\n",
    "    for param, weight in zip(model.params(), weights):\n",
    "        param[:] = weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xor loss 0.000: 100%|██████████| 3000/3000 [00:05<00:00, 562.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6425160695224108, -1.4948117798303164], [-4.56764657202966, -3.364917635073195]]\n",
      "[1.7673716823255199, 0.38727014379472763]\n",
      "[[3.1986204791704025, -3.5018030621426197]]\n",
      "[-0.6462765963362209]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR revisited\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1)\n",
    "])\n",
    "\n",
    "import tqdm\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=0.1)\n",
    "loss = SSE()\n",
    "\n",
    "with tqdm.trange(3000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
    "\n",
    "for param in net.params():\n",
    "    print(param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 64.55 acc: 0.95: 100%|██████████| 1000/1000 [08:18<00:00,  2.01it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 5.288 acc: 1.00: 100%|██████████| 100/100 [00:48<00:00,  2.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FizzBuzz Revisited\n",
    "\n",
    "from neural_networks import binary_encode, fizz_buzz_encode, argmax\n",
    "\n",
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "    Tanh(),\n",
    "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
    "    num_correct = 0\n",
    "    for n in range(low, hi):\n",
    "        x = binary_encode(n)\n",
    "        predicted = argmax(net.forward(x))\n",
    "        actual = argmax(fizz_buzz_encode(n))\n",
    "        if predicted == actual:\n",
    "            num_correct += 1\n",
    "\n",
    "    return num_correct / (hi - low)\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "loss = SSE()\n",
    "\n",
    "with tqdm.trange(1000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
    "\n",
    "# Now check results on the test set\n",
    "print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "    Tanh(),\n",
    "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n",
    "    # No final sigmoid layer now\n",
    "])\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "with tqdm.trange(100) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "        t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n",
    "\n",
    "# Again check results on the test set\n",
    "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST data\n",
    "\n",
    "#import mnist\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# This will download the data, change this to where you want it.\n",
    "# (Yes, it's a 0-argument function, that's what the library expects.)\n",
    "# (Yes, I'm assigning a lambda to a variable, like I said never to do.)\n",
    "mnist.temporary_dir = lambda: '/tmp'\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# Each of these functions first downloads the data and returns a numpy array.\n",
    "# We call .tolist() because our \"tensors\" are just lists.\n",
    "train_images = train_X.tolist()\n",
    "train_labels = train_y.tolist()\n",
    "\n",
    "assert shape(train_images) == [60000, 28, 28]\n",
    "assert shape(train_labels) == [60000]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(10, 10)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        # Plot each image in black and white and hide the axes.\n",
    "        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n",
    "        ax[i][j].xaxis.set_visible(False)\n",
    "        ax[i][j].yaxis.set_visible(False)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST test data\n",
    "\n",
    "test_images = test_X.tolist()\n",
    "test_labels = test_y.tolist()\n",
    "\n",
    "assert shape(test_images) == [10000, 28, 28]\n",
    "assert shape(test_labels) == [10000]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d0252b781bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Recenter, rescale, and flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n\u001b[0;32m----> 8\u001b[0;31m                 for image in train_images]\n\u001b[0m\u001b[1;32m      9\u001b[0m test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n\u001b[1;32m     10\u001b[0m                for image in test_images]\n",
      "\u001b[0;32m<ipython-input-24-d0252b781bbb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Recenter, rescale, and flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n\u001b[0;32m----> 8\u001b[0;31m                 for image in train_images]\n\u001b[0m\u001b[1;32m      9\u001b[0m test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n\u001b[1;32m     10\u001b[0m                for image in test_images]\n",
      "\u001b[0;32m<ipython-input-24-d0252b781bbb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Recenter, rescale, and flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n\u001b[0m\u001b[1;32m      8\u001b[0m                 for image in train_images]\n\u001b[1;32m      9\u001b[0m test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Recenter the images\n",
    "\n",
    "# Compute the average pixel value\n",
    "avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "\n",
    "# Recenter, rescale, and flatten\n",
    "train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                for image in train_images]\n",
    "test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "               for image in test_images]\n",
    "\n",
    "assert shape(train_images) == [60000, 784], \"images should be flattened\"\n",
    "assert shape(test_images) == [10000, 784], \"images should be flattened\"\n",
    "\n",
    "# After centering, average pixel should be very close to 0\n",
    "assert -0.0001 < tensor_sum(train_images) < 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test data\n",
    "\n",
    "test_labels = test_y.tolist()\n",
    "train_labels = train_y.tolist()\n",
    "\n",
    "train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "\n",
    "assert shape(train_labels) == [60000, 10]\n",
    "assert shape(test_labels) == [10000, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def loop(model: Layer,\n",
    "         images: List[Tensor],\n",
    "         labels: List[Tensor],\n",
    "         loss: Loss,\n",
    "         optimizer: Optimizer = None) -> None:\n",
    "    correct = 0         # Track number of correct predictions.\n",
    "    total_loss = 0.0    # Track total loss.\n",
    "\n",
    "    with tqdm.trange(len(images)) as t:\n",
    "        for i in t:\n",
    "            predicted = model.forward(images[i])             # Predict.\n",
    "            if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                correct += 1                                 # correctness.\n",
    "            total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "\n",
    "            # If we're training, backpropagate gradient and update weights.\n",
    "            if optimizer is not None:\n",
    "                gradient = loss.gradient(predicted, labels[i])\n",
    "                model.backward(gradient)\n",
    "                optimizer.step(model)\n",
    "\n",
    "            # And update our metrics in the progress bar.\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            acc = correct / (i + 1)\n",
    "            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mnist loss: 0.357 acc: 0.898: 100%|██████████| 60000/60000 [11:02<00:00, 90.61it/s] \n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "vectors must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3c2161bfc761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Test on the test data (no optimizer means just evaluate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-65cf7c7c1617>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(model, images, labels, loss, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Predict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;31m# Check for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m                                 \u001b[0;31m# correctness.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f5a21060968b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Return the vector of neuron outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [dot(input, self.w[o]) + self.b[o]\n\u001b[0;32m---> 24\u001b[0;31m                 for o in range(self.output_dim)]\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f5a21060968b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Return the vector of neuron outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [dot(input, self.w[o]) + self.b[o]\n\u001b[0;32m---> 24\u001b[0;31m                 for o in range(self.output_dim)]\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/Python_Data_Science_From_Scratch/linear_algebra.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(v, w)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m\"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vectors must be same length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: vectors must be same length"
     ]
    }
   ],
   "source": [
    "# The logistic regression model for MNIST\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Logistic regression is just a linear layer followed by softmax\n",
    "model = Linear(784, 10)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# This optimizer seems to work\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "\n",
    "# Train on the training data\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Test on the test data (no optimizer means just evaluate)\n",
    "loop(model, test_images, test_labels, loss)\n",
    "\n",
    "\n",
    "# A deep neural network for MNIST\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Name them so we can turn train on and off\n",
    "dropout1 = Dropout(0.1)\n",
    "dropout2 = Dropout(0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(784, 30),  # Hidden layer 1: size 30\n",
    "    dropout1,\n",
    "    Tanh(),\n",
    "    Linear(30, 10),   # Hidden layer 2: size 10\n",
    "    dropout2,\n",
    "    Tanh(),\n",
    "    Linear(10, 10)    # Output layer: size 10\n",
    "])\n",
    "\n",
    "\n",
    "# Training the deep model for MNIST\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "dropout1.train = dropout2.train = True\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Disable dropout and evaluate\n",
    "dropout1.train = dropout2.train = False\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
